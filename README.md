# SSP-SP Data Filter - Sistema Modular com Cache Inteligente

Sistema completo para scraping e an√°lise geogr√°fica de dados criminais da SSP-SP, com arquitetura modular, cache inteligente e processamento otimizado.

---

## üÜï **Novidades da Vers√£o 2.0**

### **üéØ Sistema de Cache Inteligente**
- **üìÅ Cache autom√°tico:** Rastreia arquivos e cidades j√° processados
- **‚ö° Reprocessamento inteligente:** Evita trabalho desnecess√°rio
- **üìä Controle de anos:** Bloqueia anos futuros automaticamente
- **üíæ Persist√™ncia:** Cache salvo em `cache_config.json`

### **üîÑ Novo Fluxo de Download**
- **üì• Download completo:** Baixa arquivos Excel sem filtro de cidade
- **üèôÔ∏è Filtro sob demanda:** Processa cidades apenas quando necess√°rio
- **üìÇ Estrutura organizada:** Arquivos separados por ano/categoria/cidade
- **üéØ Efici√™ncia m√°xima:** Otimiza uso de recursos e tempo

### **üìä Estrutura de Arquivos Melhorada**
```
output/
‚îú‚îÄ‚îÄ dados_criminais_2023.json          # Dados completos por ano/categoria
‚îú‚îÄ‚îÄ celulares_subtraidos_2023.json     # Dados completos
‚îú‚îÄ‚îÄ veiculos_subtraidos_2023.json      # Dados completos
‚îú‚îÄ‚îÄ objetos_subtraidos_2023.json       # Dados completos
‚îú‚îÄ‚îÄ dados_produtividade_2025.json      # Dados completos
‚îî‚îÄ‚îÄ cities/                            # Dados filtrados por cidade
    ‚îú‚îÄ‚îÄ dados_criminais_2023_S√£o_Jos√©_dos_Campos.json
    ‚îú‚îÄ‚îÄ celulares_subtraidos_2023_Campinas.json
    ‚îî‚îÄ‚îÄ veiculos_subtraidos_2023_S√£o_Paulo.json
```

---

## ‚ö†Ô∏è Requisitos do Navegador (Chromium)

O Pydoll depende de um navegador Chromium para funcionar. Por padr√£o, o Pydoll baixa e gerencia o Chromium automaticamente na primeira execu√ß√£o. **Em alguns ambientes Linux, pode ser necess√°rio instalar depend√™ncias do sistema**:

```bash
sudo apt-get update
sudo apt-get install -y chromium-browser libnss3 libatk-bridge2.0-0 libgtk-3-0 libxss1 libasound2 libgbm1 libxshmfence1 libxcomposite1 libxrandr2 libxdamage1 libxfixes3 libxext6 libx11-xcb1 libx11-6 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxi6 libxtst6 libxrandr2 libxss1 libxkbcommon0 libatspi2.0-0
```

Se o Pydoll n√£o conseguir baixar ou rodar o Chromium automaticamente, instale manualmente ou garanta que as depend√™ncias acima estejam presentes.

---

## üñ•Ô∏è Modo Headless (sem interface gr√°fica)

Por padr√£o, o scraper roda o navegador Chromium em modo **headless** (sem interface gr√°fica), tornando o scraping mais r√°pido e compat√≠vel com servidores e ambientes sem X11.

- O modo headless pode ser configurado de tr√™s formas (ordem de prioridade):
  1. **Par√¢metro ao instanciar o scraper:**
     ```python
     from utils.ssp_browser_scraper import SSPBrowserScraper
     scraper = SSPBrowserScraper(headless=False)
     ```
  2. **Configura√ß√£o centralizada em `src/config/settings.py`:**
     ```python
     # No settings.py
     PYDOLL_HEADLESS = True  # ou False
     ```
  3. **Vari√°vel de ambiente:**
     ```bash
     export PYDOLL_HEADLESS=0  # ou 1, true, false, yes, no
     ```

> **Nota:** Em servidores ou ambientes sem interface gr√°fica, mantenha o padr√£o headless=True.

---

## üèóÔ∏è Arquitetura do Projeto

```
ssp-sp-data-filter/
‚îú‚îÄ‚îÄ src/                          # C√≥digo fonte principal
‚îÇ   ‚îú‚îÄ‚îÄ core/                     # Funcionalidades principais
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ scraper.py           # Scraper principal com cache
‚îÇ   ‚îú‚îÄ‚îÄ analyzers/                # Analisadores de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ geo_analyzer.py      # Analisador geogr√°fico
‚îÇ   ‚îú‚îÄ‚îÄ config/                   # Configura√ß√µes
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ settings.py          # Configura√ß√µes centralizadas
‚îÇ   ‚îú‚îÄ‚îÄ models/                   # Modelos de dados
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ data_models.py       # Modelos estruturados
‚îÇ   ‚îú‚îÄ‚îÄ utils/                    # Utilit√°rios
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logger.py            # Sistema de logging
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ city_filter.py       # Filtro de cidades
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ geo_utils.py         # Utilit√°rios geogr√°ficos
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ file_utils.py        # Utilit√°rios de arquivo
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ cache_manager.py     # Gerenciador de cache
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ssp_browser_scraper.py # Scraper com Pydoll
‚îÇ   ‚îî‚îÄ‚îÄ __init__.py              # M√≥dulo principal
‚îú‚îÄ‚îÄ scripts/                      # Scripts execut√°veis
‚îÇ   ‚îú‚îÄ‚îÄ run_scraper.py           # Script principal com cache
‚îÇ   ‚îú‚îÄ‚îÄ geo_search.py            # Interface de busca geogr√°fica
‚îÇ   ‚îú‚îÄ‚îÄ geo_analyzer_cli.py      # An√°lise geogr√°fica CLI
‚îÇ   ‚îú‚îÄ‚îÄ test_new_system.py       # Teste do sistema de cache
‚îÇ   ‚îî‚îÄ‚îÄ setup_modular.py         # Script de setup
‚îú‚îÄ‚îÄ tests/                        # Testes
‚îÇ   ‚îú‚îÄ‚îÄ unit/                    # Testes unit√°rios
‚îÇ   ‚îî‚îÄ‚îÄ integration/             # Testes de integra√ß√£o
‚îú‚îÄ‚îÄ docs/                         # Documenta√ß√£o
‚îú‚îÄ‚îÄ examples/                     # Exemplos de uso
‚îú‚îÄ‚îÄ downloads/                    # Arquivos Excel baixados
‚îú‚îÄ‚îÄ output/                       # Arquivos JSON processados
‚îÇ   ‚îî‚îÄ‚îÄ cities/                  # Dados filtrados por cidade
‚îú‚îÄ‚îÄ cache_config.json            # Cache de processamento
‚îú‚îÄ‚îÄ requirements.txt              # Depend√™ncias
‚îî‚îÄ‚îÄ README.md                     # Documenta√ß√£o principal
```

## üöÄ Vantagens da Nova Arquitetura

### **1. Sistema de Cache Inteligente**
- **üíæ Cache persistente:** Evita reprocessamento desnecess√°rio
- **üìä Rastreamento completo:** Arquivos, cidades e anos processados
- **‚ö° Performance otimizada:** Processamento apenas do necess√°rio
- **üîÑ Controle granular:** For√ßa reprocessamento quando necess√°rio

### **2. Fluxo de Download Otimizado**
- **üì• Download completo:** Baixa todos os dados sem filtro inicial
- **üèôÔ∏è Filtro sob demanda:** Processa cidades apenas quando solicitado
- **üìÇ Estrutura organizada:** Separa√ß√£o clara de dados por categoria/ano/cidade
- **üéØ Efici√™ncia m√°xima:** Minimiza downloads e processamento

### **3. Modularidade Avan√ßada**
- **Separa√ß√£o de responsabilidades**: Cada m√≥dulo tem uma fun√ß√£o espec√≠fica
- **Reutiliza√ß√£o de c√≥digo**: Utilit√°rios compartilhados entre m√≥dulos
- **Manutenibilidade**: F√°cil de modificar e estender
- **Cache inteligente**: Sistema de cache integrado

### **4. Scraping com Pydoll**
- **Automa√ß√£o de browser**: Usa Pydoll para renderizar p√°ginas JavaScript
- **Extra√ß√£o confi√°vel**: Acessa links din√¢micos carregados via JavaScript
- **Fluxo s√≠ncrono**: Processamento sequencial e previs√≠vel
- **Fallback robusto**: Usa requests como backup para downloads
- **Sem depend√™ncias externas**: N√£o precisa de Chrome/WebDriver

### **5. Configura√ß√£o Centralizada**
- **Arquivo de configura√ß√µes**: Todas as configura√ß√µes em um local
- **Vari√°veis de ambiente**: Suporte a configura√ß√£o via env vars
- **Flexibilidade**: F√°cil de personalizar sem modificar c√≥digo

### **6. Modelos Estruturados**
- **Dataclasses**: Modelos de dados tipados e organizados
- **Valida√ß√£o**: Estruturas de dados consistentes
- **Documenta√ß√£o**: Modelos auto-documentados

### **7. Sistema de Logging**
- **Logging centralizado**: Sistema unificado de logs
- **M√∫ltiplos handlers**: Console e arquivo
- **Configur√°vel**: N√≠veis e formatos personaliz√°veis

### **8. Testes Organizados**
- **Testes unit√°rios**: Testes isolados por funcionalidade
- **Testes de integra√ß√£o**: Testes de fluxo completo
- **Cobertura**: Melhor cobertura de c√≥digo

## üõ†Ô∏è Instala√ß√£o

### **1. Clone o reposit√≥rio**
```bash
git clone https://github.com/leonardo-spy/SSP-SP-Data-Filter.git
cd SSP-SP-Data-Filter
```

### **2. Crie um ambiente virtual (recomendado)**
```bash
python3 -m venv .venv
source .venv/bin/activate  # Linux/Mac
# ou
.venv\Scripts\activate     # Windows
```

### **3. Instale as depend√™ncias**
```bash
pip install -r requirements.txt
```

> **Nota:** O Pydoll ser√° instalado via PyPI como `pydoll-python`.

### **4. Configure vari√°veis de ambiente (opcional)**
```bash
export SSP_TARGET_YEAR=2024
export SSP_DEFAULT_CITY="S√£o Jos√© dos Campos"
export SSP_REQUEST_TIMEOUT=30
export SSP_DEFAULT_RADIUS_KM=5.0
export SSP_CACHE_ENABLED=true
export PYDOLL_HEADLESS=1
```

### **5. Teste a instala√ß√£o**
```bash
# Testar o sistema de cache
python scripts/test_new_system.py

# Verificar se tudo est√° funcionando
python scripts/run_scraper.py --mostrar-cache
```

## üéØ Como Usar

### **1. Executar o Scraper com Cache Inteligente**
```bash
# Script principal (baixa dados completos)
python scripts/run_scraper.py

# Scraper com ano e cidade espec√≠ficos
python scripts/run_scraper.py --ano 2023 --cidade "S√£o Jos√© dos Campos"

# For√ßar reprocessamento (ignora cache)
python scripts/run_scraper.py --forcar-reprocessamento

# Mostrar informa√ß√µes do cache
python scripts/run_scraper.py --mostrar-cache

# Limpar cache
python scripts/run_scraper.py --limpar-cache

# Ou usando o m√≥dulo diretamente
python -c "
import sys
sys.path.append('src')
from core.scraper import SSPDataScraper

scraper = SSPDataScraper(target_year=2023, target_city='S√£o Jos√© dos Campos')
success = scraper.run()
print(f'Scraping {'sucesso' if success else 'falha'}')
"
```

### **2. An√°lise Geogr√°fica (Busca em Todos os Anos)**
```bash
# Interface interativa
python scripts/geo_search.py

# Linha de comando
python scripts/geo_analyzer_cli.py "Rua das Flores" --raio 3 --export
python scripts/geo_analyzer_cli.py "-23.5481315,-46.6375532" --raio 5 --export --output-file minha_analise.json

# Como m√≥dulo Python
python -c "
import sys
sys.path.append('src')
from analyzers.geo_analyzer import GeoAnalyzer

analyzer = GeoAnalyzer('output')
records = analyzer.search_and_analyze('S√£o Jos√©', 3.0)
analyzer.print_results(records, 'S√£o Jos√©', 3.0)
"
```

### **3. Gerenciamento de Cache**
```bash
# Verificar status do cache
python scripts/run_scraper.py --mostrar-cache

# Limpar cache (com confirma√ß√£o)
python scripts/run_scraper.py --limpar-cache

# Testar sistema de cache
python scripts/test_new_system.py

# For√ßar reprocessamento de arquivos espec√≠ficos
python scripts/run_scraper.py --ano 2023 --forcar-reprocessamento
```

### **4. Scripts de Linha de Comando**

O sistema inclui scripts para uso direto via linha de comando:

```bash
# Scraper principal com cache
python scripts/run_scraper.py

# Scraper com par√¢metros espec√≠ficos
python scripts/run_scraper.py --ano 2023 --cidade "Campinas"

# An√°lise geogr√°fica por rua
python scripts/geo_analyzer_cli.py "Rua das Flores" --raio 3 --export

# An√°lise geogr√°fica por coordenadas
python scripts/geo_analyzer_cli.py --raio 5 --export --output-file minha_analise.json -- "-23.5481315,-46.6375532"

# Gerenciamento de cache
python scripts/run_scraper.py --mostrar-cache
python scripts/run_scraper.py --limpar-cache
```

### **5. Usar como M√≥dulo Python**
```python
import sys
sys.path.append('src')

from core.scraper import SSPDataScraper
from analyzers.geo_analyzer import GeoAnalyzer
from utils.cache_manager import CacheManager
from config.settings import settings
from models.data_models import ScrapingResult, GeoRecord

# Configurar logger
from utils.logger import setup_logger
logger = setup_logger("meu_script")

# Usar configura√ß√µes
print(f"Cidade padr√£o: {settings.DEFAULT_CITY}")
print(f"Raio padr√£o: {settings.DEFAULT_RADIUS_KM}km")

# Gerenciar cache
cache_manager = CacheManager()
cache_info = cache_manager.get_cache_info()
print(f"Arquivos processados: {cache_info['total_processed_files']}")

# Executar scraping com cache
scraper = SSPDataScraper(target_year=2023, target_city='S√£o Jos√© dos Campos')
success = scraper.run()

# An√°lise geogr√°fica (busca em todos os anos dispon√≠veis)
analyzer = GeoAnalyzer('output')
records = analyzer.search_and_analyze('-23.5481315,-46.6375532', 5.0)
analyzer.print_results(records, '-23.5481315,-46.6375532', 5.0)
```

### **6. Exemplos de Uso Pr√°tico**

#### **Download Completo de Dados**
```bash
# Baixar todos os dados dispon√≠veis (sem filtro de cidade)
python scripts/run_scraper.py

# Baixar dados de um ano espec√≠fico
python scripts/run_scraper.py --ano 2023
```

#### **Processamento de Cidade Espec√≠fica**
```bash
# Processar dados para uma cidade espec√≠fica
python scripts/run_scraper.py --ano 2023 --cidade "S√£o Jos√© dos Campos"

# O sistema ir√°:
# 1. Verificar se os dados completos j√° foram baixados
# 2. Baixar se necess√°rio
# 3. Filtrar por cidade e salvar em output/cities/
```

#### **An√°lise Geogr√°fica Completa**
```bash
# Buscar por coordenadas (procura em todos os anos dispon√≠veis)
python scripts/geo_analyzer_cli.py --raio 5 --export --output-file centro_sp.json -- "-23.5481315,-46.6375532"

# Buscar por rua
python scripts/geo_analyzer_cli.py "Rua das Flores" --raio 3 --export
```

#### **Gerenciamento de Cache**
```bash
# Verificar o que j√° foi processado
python scripts/run_scraper.py --mostrar-cache

# Limpar cache se necess√°rio
python scripts/run_scraper.py --limpar-cache

# For√ßar reprocessamento
python scripts/run_scraper.py --forcar-reprocessamento
```

## ‚öôÔ∏è Configura√ß√£o

### **Arquivo de Configura√ß√µes (`src/config/settings.py`)**

```python
@dataclass
class Settings:
    # URLs e endpoints
    CONSULTAS_URL: str = "https://www.ssp.sp.gov.br/estatistica/consultas"
    
    # Configura√ß√µes de scraping
    DEFAULT_TARGET_YEAR: Optional[int] = None
    DEFAULT_CITY: str = "S√£o Jos√© dos Campos"
    REQUEST_TIMEOUT: int = 30
    MAX_RETRIES: int = 3
    CONCURRENT_REQUESTS: int = 5
    
    # Configura√ß√µes de arquivos
    DOWNLOADS_DIR: str = "downloads"
    OUTPUT_DIR: str = "output"
    LOG_FILE: str = "ssp_scraper.log"
    CACHE_FILE: str = "cache_config.json"
    
    # Configura√ß√µes de cache e controle de anos
    MAX_YEAR: int = field(default_factory=lambda: datetime.now().year)
    CACHE_ENABLED: bool = True
    FORCE_REPROCESS: bool = False
    
    # Configura√ß√µes de an√°lise geogr√°fica
    DEFAULT_RADIUS_KM: float = 5.0
    EARTH_RADIUS_KM: float = 6371.0
    
    # Configura√ß√£o do modo headless do Pydoll
    PYDOLL_HEADLESS: bool = field(default_factory=lambda: (
        os.getenv('PYDOLL_HEADLESS', '1').lower() in ['1', 'true', 'yes']
    ))
    
    # Categorias de dados
    CATEGORIES: Dict[str, str] = field(default_factory=lambda: {
        "dados_criminais": "Dados Criminais",
        "dados_produtividade": "Dados de Produtividade", 
        "morte_intervencao": "Morte Decorrente de Interven√ß√£o Policial",
        "celulares_subtraidos": "Celulares subtra√≠dos",
        "veiculos_subtraidos": "Ve√≠culos subtra√≠dos",
        "objetos_subtraidos": "Objetos subtra√≠dos"
    })
    
    # Configura√ß√µes de busca por cidade
    CITY_SIMILARITY_THRESHOLD: float = 0.7
    MIN_SIGNIFICANT_WORDS_RATIO: float = 0.6
    MIN_SIGNIFICANT_WORDS_COUNT: int = 2
    
    # Campos de coordenadas suportados (incluindo mai√∫sculas)
    LATITUDE_FIELDS: List[str] = field(default_factory=lambda: [
        'latitude', 'lat', 'coordenada_lat', 'coord_lat', 'LATITUDE'
    ])
    LONGITUDE_FIELDS: List[str] = field(default_factory=lambda: [
        'longitude', 'lon', 'lng', 'coordenada_lon', 'coord_lon', 'LONGITUDE'
    ])
    
    # Campos de endere√ßo suportados
    ADDRESS_FIELDS: List[str] = field(default_factory=lambda: [
        'endereco', 'logradouro', 'rua', 'address', 'local'
    ])
    
    # Campos priorit√°rios para exibi√ß√£o
    PRIORITY_FIELDS: List[str] = field(default_factory=lambda: [
        'tipo', 'endereco', 'logradouro', 'rua', 'local', 'descricao', 'data'
    ])
    
    # Campos secund√°rios para exibi√ß√£o
    SECONDARY_FIELDS: List[str] = field(default_factory=lambda: [
        'bairro', 'cep', 'numero', 'complemento', 'referencia',
        'periodo', 'hora', 'dia_semana', 'mes', 'ano',
        'vitima', 'suspeito', 'arma', 'veiculo', 'objeto',
        'valor', 'quantidade', 'unidade', 'observacao', 'observa√ß√µes'
    ])
```

### **Vari√°veis de Ambiente**

```bash
# Configura√ß√µes de scraping
SSP_TARGET_YEAR=2024
SSP_DEFAULT_CITY="S√£o Jos√© dos Campos"
SSP_REQUEST_TIMEOUT=30

# Configura√ß√µes de an√°lise
SSP_DEFAULT_RADIUS_KM=5.0

# Configura√ß√µes de cache
SSP_CACHE_ENABLED=true
SSP_FORCE_REPROCESS=false

# Configura√ß√µes do Pydoll
PYDOLL_HEADLESS=1
```

### **Arquivo de Cache (`cache_config.json`)**

O sistema mant√©m um arquivo de cache que rastreia:

```json
{
  "processed_files": {
    "dados_criminais_2023": {
      "category": "dados_criminais",
      "year": 2023,
      "processed_at": "2025-07-20T21:14:17.896794",
      "file_info": {
        "filename": "dados_criminais_2023.xlsx",
        "total_registros": 14839,
        "cidade_filtro": "TODAS"
      }
    }
  },
  "processed_cities": {
    "dados_criminais_2023_S√£o Jos√© dos Campos": {
      "category": "dados_criminais",
      "year": 2023,
      "city": "S√£o Jos√© dos Campos",
      "processed_at": "2025-07-20T21:14:17.898410",
      "file_info": {
        "registros_filtrados": 100,
        "total_registros": 14839
      }
    }
  },
  "available_years": [2023, 2025],
  "last_update": "2025-07-20T21:14:17.896466",
  "version": "1.0"
}
```

## üß™ Testes

### **Executar Testes**
```bash
# Testes unit√°rios
python -m pytest tests/unit/

# Testes de integra√ß√£o
python -m pytest tests/integration/

# Todos os testes
python -m pytest tests/
```

### **Cobertura de C√≥digo**
```bash
python -m pytest --cov=src tests/
```

## üìä Modelos de Dados

### **ScrapingResult**
```python
@dataclass
class ScrapingResult:
    categoria: str
    arquivo_original: str
    total_registros: int
    registros_filtrados: int
    cidade_filtro: str
    data_processamento: datetime
    dados: List[Dict[str, Any]]
    sucesso: bool = True
    erro: Optional[str] = None
```

### **GeoRecord**
```python
@dataclass
class GeoRecord:
    categoria: str
    latitude: float
    longitude: float
    distancia_km: float
    dados_originais: Dict[str, Any]
    
    def get_address(self) -> Optional[str]:
        """Extrai endere√ßo dos dados originais"""
        address_fields = ['endereco', 'logradouro', 'rua', 'local']
        for field in address_fields:
            if field in self.dados_originais:
                value = self.dados_originais[field]
                if value and str(value).strip():
                    return str(value).strip()
        return None
```

### **AnalysisResult**
```python
@dataclass
class AnalysisResult:
    query: str
    raio_km: float
    total_registros: int
    registros: List[GeoRecord]
    estatisticas: CategoryStats
    data_analise: datetime
```

## üîß Troubleshooting e FAQ

### **Problemas Comuns**

#### **1. Cache n√£o est√° funcionando**
```bash
# Verificar status do cache
python scripts/run_scraper.py --mostrar-cache

# Limpar cache se necess√°rio
python scripts/run_scraper.py --limpar-cache

# For√ßar reprocessamento
python scripts/run_scraper.py --forcar-reprocessamento
```

#### **2. An√°lise geogr√°fica n√£o encontra dados**
- **Verificar se os arquivos JSON existem:**
  ```bash
  ls -la output/*.json
  ```
- **Verificar se as coordenadas est√£o nos dados:**
  ```bash
  python -c "import json; data=json.load(open('output/dados_produtividade_2025.json')); print('Primeiras coordenadas:', [(r.get('LATITUDE'), r.get('LONGITUDE')) for r in data['dados'][:3] if r.get('LATITUDE')])"
  ```

#### **3. Erro de importa√ß√£o de m√≥dulos**
```bash
# Verificar se est√° no ambiente virtual
source .venv/bin/activate

# Verificar se as depend√™ncias est√£o instaladas
pip list | grep pydoll
```

#### **4. Pydoll n√£o consegue baixar Chromium**
```bash
# Instalar depend√™ncias do sistema (Linux)
sudo apt-get update
sudo apt-get install -y chromium-browser libnss3 libatk-bridge2.0-0 libgtk-3-0 libxss1 libasound2 libgbm1 libxshmfence1 libxcomposite1 libxrandr2 libxdamage1 libxfixes3 libxext6 libx11-xcb1 libx11-6 libxcb1 libxcomposite1 libxcursor1 libxdamage1 libxi6 libxtst6 libxrandr2 libxss1 libxkbcommon0 libatspi2.0-0
```

### **FAQ**

#### **Q: Por que o sistema baixa dados completos em vez de filtrar por cidade?**
**A:** O novo fluxo otimiza a efici√™ncia:
- Baixa dados completos uma vez
- Filtra por cidade sob demanda
- Evita reprocessamento desnecess√°rio
- Permite an√°lise geogr√°fica em todos os dados

#### **Q: Como o cache funciona?**
**A:** O sistema mant√©m um arquivo `cache_config.json` que rastreia:
- Arquivos j√° processados (por categoria/ano)
- Cidades j√° filtradas (por categoria/ano/cidade)
- Anos dispon√≠veis
- Metadados de processamento

#### **Q: Posso for√ßar o reprocessamento?**
**A:** Sim, use a flag `--forcar-reprocessamento`:
```bash
python scripts/run_scraper.py --forcar-reprocessamento
```

#### **Q: A an√°lise geogr√°fica busca em todos os anos?**
**A:** Sim! O sistema carrega todos os arquivos JSON dispon√≠veis e busca em todos os anos automaticamente.

#### **Q: Como limpar o cache?**
**A:** Use o comando:
```bash
python scripts/run_scraper.py --limpar-cache
```

#### **Q: O sistema bloqueia anos futuros?**
**A:** Sim, automaticamente. O sistema n√£o permite processar anos maiores que o ano atual.

### **Logs e Debugging**

#### **Verificar logs**
```bash
# Ver logs em tempo real
tail -f ssp_scraper.log

# Ver √∫ltimos 50 linhas
tail -50 ssp_scraper.log
```

#### **Modo debug**
```bash
# Ativar modo debug
export SSP_DEBUG=1
python scripts/run_scraper.py
```

#### **Testar componentes individuais**
```bash
# Testar sistema de cache
python scripts/test_new_system.py

# Testar an√°lise geogr√°fica
python scripts/geo_analyzer_cli.py --raio 1 -- "-23.5481315,-46.6375532"
```

## üìà Performance e Otimiza√ß√µes

### **Cache Inteligente**
- **Evita reprocessamento:** Arquivos j√° processados s√£o pulados
- **Filtro sob demanda:** Cidades processadas apenas quando necess√°rio
- **Controle granular:** For√ßa reprocessamento quando necess√°rio

### **Estrutura de Arquivos Otimizada**
- **Dados completos:** Um arquivo por categoria/ano
- **Dados filtrados:** Subdiret√≥rio `cities/` para dados por cidade
- **Organiza√ß√£o clara:** F√°cil de navegar e gerenciar

### **An√°lise Geogr√°fica Eficiente**
- **Busca em todos os anos:** Carrega todos os dados dispon√≠veis
- **Campos de coordenadas flex√≠veis:** Suporta mai√∫sculas e min√∫sculas
- **C√°lculo de dist√¢ncia otimizado:** Algoritmo eficiente de dist√¢ncia

## ü§ù Contribuindo

### **Como Contribuir**
1. Fork o reposit√≥rio
2. Crie uma branch para sua feature (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudan√ßas (`git commit -am 'Adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

### **Padr√µes de C√≥digo**
- Use type hints em todas as fun√ß√µes
- Documente fun√ß√µes e classes
- Mantenha testes atualizados
- Siga o padr√£o de nomenclatura do projeto

### **Testes**
```bash
# Executar todos os testes
python -m pytest tests/

# Executar com cobertura
python -m pytest --cov=src tests/

# Executar testes espec√≠ficos
python -m pytest tests/unit/test_cache_manager.py
```

## üìÑ Licen√ßa

Este projeto est√° licenciado sob a MIT License - veja o arquivo [LICENSE](LICENSE) para detalhes.

## üôè Agradecimentos

- **SSP-SP** pela disponibiliza√ß√£o dos dados
- **Pydoll** pela automa√ß√£o de browser
- **Comunidade Python** pelas bibliotecas utilizadas

---

**Desenvolvido com ‚ù§Ô∏è para an√°lise de dados de seguran√ßa p√∫blica** 